\chapter{Technologia CUDA}
\section{Współbieżna przyszłość}
Kiedy w roku 2005 H. Sutter \cite{lunch} opublikował artykuł o intrygująco 
brzmiącej nazwie 'Koniec darmowego lanczu', w szeroko pojętym środowisku 
deweloperskim rozgorzała dyskusja. Autor stwierdził, że obserwujemy kres 
wykładniczego wzrostu wydajności mikroprocesorów, rozumianego przez wzrost 
częstotliwości taktowania ich zegarów w zależności od ilości użytych
tranzystorów. Z tym argumentem, nie można się nie zgodzić patrząc na
oferowane na rynku procesory firmy Intel przedstawione na rysunku \ref{proce}.

\begin{figure}[ht]\label{proce}
\centering
\includegraphics[scale=1.0]{images/CPU.png}
\caption{Źródło: http://www.gotw.ca/}
\end{figure}

Ważniejszą jednak tezą postawioną przez Suttera było stwierdzenie, że
programiści nie będą mogli dłużej korzystać ze wzrostu mocy obliczeniowej
sprzętu. Taki wzrost wydajności często okazywał się dla twórców aplikacji
niezastąpiony. Napotykając problemy wydajnościowe w swoim oprogramowania
programiści mogli albo poświęcić się żmudnemu procesowi optymalizacji, albo
po prostu podnieść jej wymaganie sprzętowe. Często, głównie ze względów
ekonomicznych drugi wariant było wybierany, gdyż ograniczał się do tylko do
poczekania na nowe generacje sprzętu. To zjawisko, ciekawie scharakteryzował J.
Spolsky przytaczany w \cite{nolunch} ,,Jako programista masz wybór, albo
spędzisz pół roku na przedesignowaniu swojej aplikacji, wstawiając kod asemblera
w krytycznych sekcjach, albo na wakacjach, grając na perkusji w rockowej kapeli.
Niezależnie od alternatywy którą wybierzesz, twoja aplikacji będzie działała
szybciej,,.

Czy jednak założenie o niskim wzroście wydajności współczesnym mikroprocesorów
jest prawdziwe? Wprawdzie częstotliwość taktowania nie podlega już takim trendom
co wcześniej, jednak dzisiejsze architektury CPU oferują więcej niż jeden rdzeń
zdolny do wykonywania programu. Obecnie na rynku jest już oferowany procesor
Intel z serii i7, który może posiadać do 8 fizycznych rdzeni. Dodatkowo, nowe
technologie takie jak hyperthreading, pipelining czy zaawansowany brach
prediction pozwalają na możliwie szybkie, czy wręcz równoległe wykonywanie
fragmentów sekwencyjnego kodu.

Mimo nowoczesnej architektury CPU, sekwencyjnie programy i tak wykonywane są
tylko na pojedynczym rdzeniu\cite{massive}, który jak było to opisane wcześniej,
na przestrzeni ostatnich lat stał się znacząco szybszy. Nawet 
procesory posiadające instrukcje typu SIMD oraz heurystyki wykorzystywane w
nowoczesnych kompilatorach, takie jak np. 'Loop Vectorizer' wykorzystywany w
kompilatorach bazujących na LLVM \cite{llvm} czy 'Auto-Vectorization' w GCC
\cite{gcc} nie są w stanie zamienić sekwencyjnego kodu w wydajny kod
współbieżny.

Sutter stwierdza, że odpowiedzą na postawiony wyżej problem jest zmiana
paradygmatu z programowania sekwencyjnego na współbieżne. Tworzone wielowątkowe
aplikacje będą w stanie korzystać z wielordzeniowych architektur, co przyspieszy
ich wykonywanie a programistom pozwoli nadal oczekiwać na ,,darmowy lancz''.
Faktem potwierdzającym postawiona przez niego tezę jest rysunek \ref{gflops},
przedstawiający teoretyczną maksymalną wydajność wielo-rdzeniowych mikroprocesorów oraz układów
graficznych mierzoną w giga FLOPS-ach (operacji zmienno-przecinkowych na
sekundę).

\begin{figure}[ht]\label{gflops}
\centering
\includegraphics[scale=0.4]{images/floating-point-operations-per-second.png}
\caption{Źródło: CUDA C Programming Guide}
\end{figure}

Samo jednak przejście na model programowania równoległego nie będzie łatwe,
przyjemne, a przede wszystkim tanie. Wg
Suttera taka zmiana wiązać się będzie nie tylko ze zmianą architektury
aplikacji, lecz też systemu operacyjnego czy konstrukcjami języków
programowania. 

Zmiany w stronę wielowątkowości obserwowane są jakiegoś czasu. W nowym
standardzie języka C++ 11, biblioteka obsługująca wątki będzie wchodzić w skład
biblioteki standardowej, Microsoft publikuje wielowątkowe wersje popularnych
bibliotek dla Platformy .NET jak PLINQ, a NVIDIA biblioteki algorytmiczne
(CUFFT) wykonywane na GPU. Marsz w stronę wielowątkowości obserwujemy cały czas,
	ale nie będzie to jednak rewolucja zapowiadana w \cite{rewolucja}, lecz moim
	zdaniem bardziej ewolucja. Warto dodać, że dużo pracy zostało już wykonane.
	Serwery www oraz bazy danych są świetnym przykładem wielowątkowych
	aplikacji.

Koleją istotną kwestią w projektowaniu wielowątkowych aplikacji jest jej
skalowalność. Jeżeli dany problem programistyczny nie będzie w stanie być
dynamicznie dzielony na podproblemy, które będą mogły być rozwiązany
indywidualnie, korzyści związane z przyrostem ilości rdzeni w sprzęcie nie będą
zauważane. Taki podział często okazuje się być nietrywialny, a czasem
niemożliwy. Nie można też oczekiwać wielkich wzrostów wydajności, ponieważ nie
cały kod aplikacji może być zrównoleglony. Dobrze opisuje to formuła stworzona w
1967 r. przez G. Amdahl.

\begin{equation}
W(N) = \frac{1}{(1-S) + \frac{S}{N}}
\end{equation}
,gdzie $N$ jest ilością jednostek wykonywania, a $S$ jest \% kodu programu, który może być zrównoleglony. I tak dla 8 rdzeni i programu i współczynnika $S=60\%$ otrzymujemy wzrost wydajności około 2.1 raza.

Współbieżność jest bez wątpienia problemem z którym każdemu programiście
przyjdzie się kiedyś zmierzyć. Możliwe, że do niektórych problemów wystarczą mu
gotowe rozwiązania z dostępnych bibliotek, jednak myślenie o problemie i
przedstawienie go w postaci dającej się zrównoleglić będzie rzeczą
najważniejszą. Środowiska naukowe pomagają w tym aspekcie bardzo istotnie.
Każdego roku publikowane są artykuły przedstawiające często nowatorskie
podejścia do zagadnień wskazując możliwość ich współbieżnego rozwiązania.
Oczekuję, że w następnych latach trend z programowaniem równoległym będzie
przybierał na sile, czego owocem będą nowe, innowacyjne metody i technologie.

\section{Powstanie CUDA}

Technologia CUDA (Compute Unified Device Architecture) została po raz pierwszy
zaprezentowana przez NVIDIA w listopadzie 2006 r. Przedstawiła ona nowy model
programowania aplikacji w którym sekwencyjne fragmenty kodu są wykonywane na
CPU, natomiast te wymagające obliczeniowo, na procesorach graficznych (GPU).
Pierwsze karty graficzne z serii GeForce 8800, implementujące technologię CUDA,
		 pojawiły się w roku 2006 r. Programiści od tego czasu mogą korzystać ze
		 specjalnie zaprojektowanych w tym celów interfejsów programistycznych
		 bibliotek CUDA.

Sama koncepcja programowania procesorów graficznych jest znana od dawna.
Programiści używając interfejsów do programowania shaderów, dostępnych chociażby
w OpenGL 1.4 (2002) czy Direct3D 8.0 (2001), mogli dokonywać równoległych
obliczeń na kartach graficznych. Wymagało to jednak często wielu trików, takich
jak przekazywanie danych poprzez tekstury czy odczytywania danych wyjściowych z
wygenerowanej ramki obrazu. NVIDIA wyszła naprzeciw tym problemom, tworząc
dedykowane na ten cel interfejsy programistyczne napisane w C i C++.

Na sukces technologi CUDA złożyło się wg \cite{massive} parę czynników.
Pierwszym jest fakt, że programiści aplikacji równoległych otrzymali środowisko
w którym ich kod, wg zapewnień NVIDIA, będzie wykonywany poprawnie, niezależnie
od używanego sprzętu. Ma to szczególnie ważne znaczenie, biorąc pod uwagę fakt,
   że projektanci kart graficznych nie zakładali początkowo ich użycia do
   obliczeń inżynierskich. I tak np. kalkulacje na liczbach zmiennoprzecinkowych
   na różnych kartach graficznych NVIDIA do 2006 r. mogły skutkować innymi
   wynikami. Dopiero specyfikacja technologii CUDA wymusiła na projektantach
   sprzętu zgodność ze standardami publikowanymi przez IEEE.

Kolejnym czynnikiem, który zdecydował o sukcesie technologii CUDA jest
dostępność medium, na którym wielowątkowe, zrównoleglone aplikacje mogą być
wykonywane. W chwili obecnej na rynku znajdują się setki milionów kart
graficznych wyprodukowanych przez NVIDIA zdolnych do wykonania kodu napisanego w
CUDA. Ma to bardzo istotny wymiar ekonomiczny, ponieważ wiele specjalistycznych
(np. w medycynie) aplikacji nie musi być dostarczana wraz z drogim,
	dedykowanym dla tego celu sprzętem. Spowodowało to zatem wzrost rynku dla
	tego typu rozwiązań i stworzyło ekonomiczne uzasadnienie do dalszej pracy
	nad współbieżnie wykonywanymi aplikacjami.

Ostatnim, najbardziej oczywistym czynnikiem, jest wzrost wydajności. Procesory
graficzne składające się z multiprocesorów strumieniowych są przystosowane do
przetwarzania dużej ilości danych jednocześnie. W nowych architekturach takich
jak GeForce z serii 680 posiadającą aż 1536 rdzeni zdolnych do równoległego
wykonywania kodu. Efektem tego może być wzrost wydajności aplikacji w niektórych
zastosowaniach nawet do 150 razy \cite{prez}.

\section{Interfejs programisty}

\subsection{Kompilacja}

Zaproponowany we frameworku CUDA model programowania zakłada możliwość
skompilowania kodu w dwóch różnych kontekstach - hosta (CPU) oraz device'a
(procerora graficznego). Jako kontekst rozumiany jest specyficzny dla danej architektury zestaw
instrukcji dla procesora. Fragmenty programu, których zrównoleglenie jest
niemożliwe są tworzone w kontekście hosta, natomiast te wymagające intensywnych,
wielowątkowych obliczeń w kontekście device'a. Współistnienie dwóch
kontekstów w jednym programie wykonywalnym możliwe jest dzięki zestawie
bibliotek i narzędzi dostarczanych wraz z pakietem CUDA.

Biblioteki dostarczane wraz z pakietem CUDA umożliwiają pisanie kodu CUDA w różnych
językach programowani. Najczęściej stosowanych jest język C, istnieją jednak bibioteki
dla języków C++, Java, Python, C czy Fortran. Dodatkowo kod CUDA może być też pisany w specjalnie stworzonym do tego celu
niskopoziomowym języku PTX (Pararell Thread Execution), przypominającym asembler.
Cechą wyróżniającą kod PTX od skompilowanego kodu binarnego, jest możliwość jego
kompilacji w locie z użyciem tzw. Just-In-Time komilatora dostarczanego przez
NVIDIĘ wraz ze sterownikami do karty graficznej. Pozwala to na uruchamianie kodu
CUDA nie tylko na różnych wersjach sterowników karty graficznej, lecz również na
innych procesorach graficznych firmy NVIDIA bez potrzeby rekompilacji.

Niezależnie od wybranego języka programowania ostatecznie kod programu CUDA musi
zostać skompilowany do postaci binarnej i uruchomoiony w kontekście hosta. W
czasie działania programu wykonywanego sekwencyjnie na CPU, następuje zmiana
kontekstów i uruchomienie skompilowanych fragmentów kodu na procesorze graficznym.
Przełączanie konteksów jest transparentne dla programisty,
ponieważ ogranicza się tylko do wywołania odpowiednich funkcji z bibliotek
dostarczanych wraz z pakietem CUDA.

CUDA to nie tylko zestaw bibliotek, lecz również zestaw rozszerzeń do języków
programowania. Istnienie rozszerzeń dla języka C, jest
podyktowane faktem, że składania C jest językiem wystarczającym do opisu kodu
wykonywanego w kontekście device'a oraz potrzebą jednoznacznego rozróżnienia
fragmentów programu każdego z kontekstów w kodzie źródłowym. W rezultacie
można więc pisać kod CUDA używając ANSI C oraz specjalnych rozszerzeń
przypominających znane wszystkim atrybuty kompilatora. W ten sposób NVIDIA
zaoszczędziła programistom CUDA potrzeby nauki nowego języka programowania, czy
pisania kodu w niskopoziomowym języku PTX. Użycie specjalistycznych atrybutów
kompilacji dla języka C, możliwe jest tylko dzięku wykorzystaniu dostarczaniego wraz z
frameworkiem CUDA kompilatora 'nvcc'.

NVIDIA Cuda Compiler (nvcc) nie jest kompilatorem w pełnym tego słowa
znaczeniu, a bardziej zestawem narzędzi służących do kompilacji. W przypadku gdy
kod źródłowy będzie kompilowany tylko w kontekście hosta, właściwą kompilacją
zajmują się kompilatory dedykowane dla danego systemu operacyjnego. I tak dla
Windows, nvcc wykorzystuje kompilator Visual Studio, natomiast dla systemu Linux
użyty jest m.in. GCC. Zadaniem NVCC jest ukrycie przed programistą etapów kompilacji
specyficznego kodu CUDA dla konkretnej implementacji, którą jest procesor
graficzny. Schemat kompilacji wraz z poszczególnymi etapami przedstawia rysunek

NVCC używane jest zazwyczaj do kompilacji kodu źródłowego do postaci binarnej.
Możliwe jest natomiast użycie kompilatora NVIDIA do kompilacji kodu napisanego w
C do postaci PTX, kompilacji do postaci cubin - kodu binarnego CUDA
przeznaczonego dla określonego procesora graficznego. NVCC wspiera też inne
standardowe etapy kompilacji takie jak preprocessing, generowanie plików
objektów czy linkowanie.

Pliki źródłowe programu napisanego w języku C zawierające rozszerzenia CUDA, przyjęło się zwyczajowo
sufiksować końcówką ".cu", aby odróżnić je od konwencjonalnych programów C
oznaczanuch sufiksem ".c". Oba rodzaje rozszerzeń są akceptowane przez NVCC, co
pozwala na wykorzstanie do kompilacji projektów CUDA tylko jednego kompilatora.
W celu ułatwienia procesu budowania NVCC wspiera większość tradycyjnych
parametrów kompilatora znanych z GCC czy Visual Studio Compiler. 

\subsection{Kernele}
Z punktu widzenia programisty wykonanie fragmentu kodu w kontekście device'a
ogranicza się do wywołania specyficznego rodzaju funkcji, nazywanego w nomenklaturze CUDA kernelami.
Kernele we języku C oznaczone są specjalnym atrybutem kompilatora device oraz mają
specyficzną konwencję wywołania przedstawioną na XXX.

Funkcja kernela wykonywana jest na procesorze graficzny jednocześnie przez N
różnych wątków. W celu określenia dokładnej ilości wątków dla których ma się ona
wykonać oraz sposobu indeksowania poszczególnych wątków dwa dodatkowe parametry
są przekazane do funkcji między znacznikami <<< oraz >>>. Pierwszy parametr
oznacza 

\subsection{Hierarchia pamieci}



\begin{figure}[ht]
\centering
\input{images/cuda}
\caption{CUDA}
\label{cuda-model}
\end{figure}

\section{GPU}

Procesory graficzne od początku swojej historii projektowane były z myślą o
wykonywaniu współbieżnym dużej ilości obliczeń na liczbach zmiennoprzecikowych. 
Wykorzystywane były głównie do akceleracji grafiki 3D, jednak wraz ze wzrostem mocy obliczeniowej
pojawiło się zapotrzebowanie na wykorzystanie ich w bardziej
generycznych rozwiązaniach. Aby to było możliwe procesory graficzne musiały
zostać zmodyfikowane, aby sprostać postawionym przed nimi nowym zadaniom.
Pierwsza modyfikacja obejmowała wprowadzenie standardu IEEE 754-2008 dla
operacji zmiennoprzecinkowych. Następne to wprowadzenie liczb
zmiennoprzecinkowych podwójnej precyzji w serii Nvidii Tesla, zwiększenie ilości
pamięci podręcznej L1 dla multiprocesorów czy większa liczba instrukcji
możliwych do wykonania w każdym wątku.

Procesor graficzny mimo swojej coraz to bardziej zaawansowanej budowy nadal
znacznie różni się od architektury CPU. Przedstawia to rysunek \ref{cpugpu}, na
którym widać podział użytych tranzystorów pomiędzy logiczne moduły procesora.
Wynika z niego, że procesory graficzne nadal posiadają mniejszą ilość pamięci cache oraz
mniej skomplikowane sterowanie przepływem instrukcji niż ich Intelowi
odpowiednicy. Wyróżniają się natomiast ilością jednostek arytmetyczno-logicznych
(ALU) dostępnych na układzie scalonym. Taka różnica sprawia że procesory te są wyspecjalizowane w rozwiązywaniu
problemów wymagających dużej intensywności obliczeniowej mierzonej jako stosunek
operacji arytmetycznych do operacji na pamięci. Przykładem takich zdań jest nie tylko 
renderowanie grafiki 3D, lecz również przetwarzanie obrazów, kodowanie i
dekodowanie video, stereo wizja, rozpoznawanie wzorców czy przetwarzanie sygnałów.

Współbieżne wykonywanie dużej ilości zadań jednoczśnie możliwe jest dzięki 
zastosowaniu architektury SIMT (Single Intruction Multiple Thread), która
przypomina architekturę SIMD używaną w współczesnych procesorach. W
odróżnieniu od SIMD, który udostępnia tylko możliwość równoległego wykonania danej
opracji na wektorze danych, SIMT umożliwia też równoległe wykonywanie wielu
niezależnych wątków. Zaletą wykonywania wątków, zamiast elementarnych operacji jest większa
elastyczność w sterowaniu przepływem programu. Każdy wykonywany wątek
na procesorze graficznym posiada własny licznik programu, dzięki czemu możliwe
jest wykonanie innych zestawów instrukcji procesora (tzw. branching).

\begin{figure}[ht]\label{cpugpu}
\centering
\includegraphics{images/gpu-devotes-more-transistors-to-data-processing.png}
\caption{Podział tranzystorów w architekturze CPU i GPU. Źródło: CUDA C Programming Guide}
\end{figure}

\subsection{Architektura procesorów graficznych}

W rozdziale tym omówiona zostanie architektura procesora graficznego
w kontekście 
Multiprocesor to według ogólnej definicji systemem, który zawiera więcej niż
jeden procesor\cite{multi}. Każdy multiprocesor składa się z procesorów
przeznaczonych do obliczeń, pamięci cache

Multiprocesor - z czego się składa 

Warpy - podstawowa jednostka

Branching - konsekwencje

\begin{figure}[ht]
\centering
\includegraphics[scale=0.8]{images/gpu.png}
\caption{Źródło: CUDA Manual}
\end{figure}

\subsection{Device capabilities}
sasda
